{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVzor0LcVzZtMmGTP3cVnd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorisBartesaghi/Kernel-Perceptron/blob/main/Frequent_itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqkhEasR-Iuk"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload() #import the kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import dataset\n",
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d alvations/old-newspapers\n",
        "!mkdir assignment\n",
        "!unzip old-newspapers.zip -d assignment"
      ],
      "metadata": {
        "id": "KWPbzcPY-cb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install pyspak\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "eXF8kuoa-9Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initiate a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.execution.arrow.fallback.enabled\", \"true\") \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "tjF_6YCl-9aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create an object containing the dataset downloaded from the kaggle dataset\n",
        "newspapers = '/content/assignment/old-newspaper.tsv'"
      ],
      "metadata": {
        "id": "ozjVHfLhAAHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA STRUCTURE UNDERSTANDING**"
      ],
      "metadata": {
        "id": "aR959hO-j_SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#understand the schema of the dataset\n",
        "df = spark.read \\\n",
        "    .options(header = True, sep= r'\\t') \\\n",
        "  .csv(newspapers)\n",
        "df.schema"
      ],
      "metadata": {
        "id": "5GxleSUzArih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the text contained in each line\n",
        "df2 = df.where(df.Language == 'English').select('Text')\n",
        "df2.select('Text').limit(5).toPandas()\n"
      ],
      "metadata": {
        "id": "f75ZcAEPJCsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA PREPROCESSING**"
      ],
      "metadata": {
        "id": "Mi2HmdmlkJhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataRDD = spark.sparkContext.textFile(newspapers)"
      ],
      "metadata": {
        "id": "-slt-Z4aCT2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here I'm creating my key value pairs\n",
        "def extract_text(file):\n",
        "  line = file.split('\\t')\n",
        "  if line[0] == 'English':\n",
        "    return line[3]\n",
        "  else:\n",
        "    return 'Pay attention, not English'\n"
      ],
      "metadata": {
        "id": "n65Gs04iKsYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TextRDD = dataRDD.map(extract_text)"
      ],
      "metadata": {
        "id": "mtgvNp3oNUlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "English_TextRDD = TextRDD.filter(lambda line: line != \"Pay attention, not English\")"
      ],
      "metadata": {
        "id": "VpJHXxplPhRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "English_TextRDD = English_TextRDD.map(lambda s: s.lower().replace('p.m.',''))\n",
        "English_TextRDD = English_TextRDD.map(lambda s: s.lower().replace('a.m.',''))"
      ],
      "metadata": {
        "id": "RwBjNQyWo1m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(line):\n",
        "    line1 = line.lower()\n",
        "    punc = '!#$%&\\+()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789'\n",
        "    line1 = line1.strip('\"')\n",
        "    for ch in punc:\n",
        "        line1 = line1.replace(ch, '')    \n",
        "    return line1"
      ],
      "metadata": {
        "id": "Q0-5KsLCTira"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cleaned_RDD = English_TextRDD.map(split)"
      ],
      "metadata": {
        "id": "QVP-oLToVIAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "remover = StopWordsRemover()\n",
        "stopwords = remover.getStopWords()\n",
        "\n",
        "def retain_unique(line):\n",
        "  words = line.split()\n",
        "  new_line = []\n",
        "  for word in words:\n",
        "    if (line.count(word) > 1 and (word not in new_line) or line.count(word) == 1) and word not in stopwords:\n",
        "        new_line.append(word)\n",
        "    else:\n",
        "        pass\n",
        "  return new_line"
      ],
      "metadata": {
        "id": "AWBxY09NijRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Listed_RDD = Cleaned_RDD.map(retain_unique)\n"
      ],
      "metadata": {
        "id": "sYb5HhJDnJla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA ANALYSIS**\n",
        "\n"
      ],
      "metadata": {
        "id": "KGK4OjJ7Sz4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the next step is to create a unique array for counting the support for each word present in our analysis\n",
        "items = Listed_RDD.flatMap(lambda line:line)"
      ],
      "metadata": {
        "id": "fBNREzAG8n59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Unique frequent items in dataset\n",
        "uni_item = items.distinct()"
      ],
      "metadata": {
        "id": "5Ufnsy8FCjf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method for sum in reduceByKey method\n",
        "def sumOparator(x,y):\n",
        "    return x+y"
      ],
      "metadata": {
        "id": "Kant5UgTfFUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 1 as Tuple\n",
        "supportRdd = items.map(lambda item: (item , 1))\n",
        "supportRdd = supportRdd.reduceByKey(sumOparator)\n",
        "\n",
        "#return only the frequncy of each word\n",
        "supports = supportRdd.map(lambda item: item[1])\n"
      ],
      "metadata": {
        "id": "NMNFxcF2yVtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "cols = StructType([       \n",
        "    StructField('item', StringType(), True),\n",
        "    StructField('counting', IntegerType(), True)\n",
        "])\n",
        "deptDF = spark.createDataFrame(data = supportRdd, schema = cols)"
      ],
      "metadata": {
        "id": "l38ENjcHN7tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "(deptDF\n",
        "    .agg(\n",
        "        F.avg(F.col('counting')).alias('average frequence'),\n",
        "        F.min(F.col('counting')).alias('minimin frequence'),\n",
        "        F.max(F.col('counting')).alias('maximum frequence'),\n",
        "    )\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "id": "nJuDVxsRH4Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deptDF.select('item').distinct().count()"
      ],
      "metadata": {
        "id": "wDMtZ8hg4CsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deptDF.agg(F.sum(\"counting\")).collect()[0][0]"
      ],
      "metadata": {
        "id": "2eX_Y7rTOOJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot \n",
        "new = deptDF.sort(deptDF.counting.desc()).take(10)\n",
        "for_barplot  = spark.createDataFrame(new,['word','counting'])\n",
        "\n",
        "word_count = for_barplot.collect()\n",
        "#create a numeric value for every label\n",
        "indexes = list(range(len(word_count)))\n",
        "\n",
        "#split words and counts to different lists \n",
        "values = [r['counting'] for r in word_count]\n",
        "labels = [r['word'] for r in word_count]\n",
        "\n",
        "#Plotting\n",
        "bar_width = 0.35\n",
        "\n",
        "pyplot.bar(indexes, values)\n",
        "\n",
        "#add labels\n",
        "labelidx = [i + bar_width for i in indexes] \n",
        "pyplot.xticks(labelidx, labels)\n",
        "\n",
        "fig1 = pyplot.gcf()\n",
        "\n",
        "pyplot.show()\n",
        "fig1.savefig('/content/frequence1.png', bbox_inches = 'tight')\n"
      ],
      "metadata": {
        "id": "z3LwFxv_ijfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#files.download('frequence.png')"
      ],
      "metadata": {
        "id": "Rn5Q3VcZgOAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BUILT-IN FUNCTIONS**"
      ],
      "metadata": {
        "id": "Oq_11aCckmoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "APRIORI ALGORITHM\n"
      ],
      "metadata": {
        "id": "p8DWjP9DdA-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "oJSFGaZDJgPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_basket = Listed_RDD.collect()\n"
      ],
      "metadata": {
        "id": "7Th9cURgKp-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try_set = words_basket[0:10000]"
      ],
      "metadata": {
        "id": "LtSQIwA2OSfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "te = TransactionEncoder()\n",
        "Array = te.fit(try_set).transform(try_set, sparse=True)\n",
        "sparse_df = pd.DataFrame.sparse.from_spmatrix(Array, columns=te.columns_)\n",
        "res= apriori(sparse_df, min_support=0.005, use_colnames=True)"
      ],
      "metadata": {
        "id": "N1auj8OlJttv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_items =res[res.itemsets.apply(lambda x: len(x) > 1)]\n",
        "res_items.sort_values(by=['support'],ascending=False).head(10)"
      ],
      "metadata": {
        "id": "OOpafAbqQB50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FPGROWTH\n"
      ],
      "metadata": {
        "id": "H0FnLsAVdJ1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.fpm import FPGrowth"
      ],
      "metadata": {
        "id": "RBEbonzUIwvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = Listed_RDD.map(lambda x: [x]).toDF(['words'])\n",
        "df = df.sample(False, 0.1, seed=0)"
      ],
      "metadata": {
        "id": "OLgxqhT4I0Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpGrowth = FPGrowth(itemsCol='words', minSupport= 0.005, minConfidence = 0.001, numPartitions= 1000)\n",
        "model2 = fpGrowth.fit(df)"
      ],
      "metadata": {
        "id": "m86xcDGiI6gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_model = model2.associationRules\n",
        "find_model.sort(find_model.support.desc()).show(20)"
      ],
      "metadata": {
        "id": "oAOLtyVFJHsU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}