{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "data = pd.read_csv(\"mnist_train.csv\")\n",
    "data_test = pd.read_csv(\"mnist_test.csv\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the training set, create the first two dataset: WILL BE USED IN HYPERPARAMETERS OPTIMIZATION \n",
    "#yt is the column of the starting training set indicating the digit (0,1,2,3,4,5,6,7,8,9)\n",
    "#xt are the columns in which are expressed the valuesof each\n",
    "yt = data['label']\n",
    "xt = data.loc[:, data.columns != 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the training set, create the second group of two dataset: wILL BE USED TO TRAINING THE ALGORITHM AND OUTPUT THE PREDICTOR \n",
    "#yT is the column of the starting training set indicating the digit (0,1,2,3,4,5,6,7,8,9)\n",
    "#xT are the columns in which are expressed the values of each pixel\n",
    "yT = data['label']\n",
    "xT = data.loc[:, data.columns != 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the test set, create last two dataset: wILL BE USED FOR TESTING\n",
    "#yT is the column of the test set indicating the digit (0,1,2,3,4,5,6,7,8,9)\n",
    "#xT are the columns in which are expressed the values of each pixel\n",
    "y_tst = data_test['label']\n",
    "x_tst = data_test.loc[:, data_test.columns != 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the function train_test_split to create the TRAINING SET and the DEVELOPMENT SET. \n",
    "#They will be necessary for hyperparameters optimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(xt, yt, test_size=0.3, stratify = yt, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarize the label, in order to compute binary classificators for each digit\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_train = y_train.astype('int8')\n",
    "y_dev = pd.get_dummies(y_dev)\n",
    "y_dev = y_dev.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change from zero to -1 the label\n",
    "y_train[y_train != 1] = -1\n",
    "y_dev[y_dev != 1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to np array, easier object to work with\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "x_dev = x_dev.to_numpy()\n",
    "y_dev = y_dev.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each entries must be normalized\n",
    "mean_px = x_train.mean().astype(np.float32)\n",
    "std_px = x_train.std().astype(np.float32)\n",
    "x_train = (x_train - mean_px)/(std_px)\n",
    "\n",
    "#use a smaller dataset to decrease the time of computation\n",
    "x_train = x_train[:6000,:]\n",
    "x_dev = x_dev[:2000,:]\n",
    "y_train = y_train[:6000,:]\n",
    "y_dev = y_dev[:2000,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the gram matrix, to insert in the function for computing the classifier\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import sign\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import gram_poly\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import kernel_poly\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import find_predictors\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import define_bests\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import mean_classifier\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import accuracy_test\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import accuracy_train\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import kernel_perceptron\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import confusion_matrix\n",
    "from ipynb.fs.full.Basic_functions_Kernel_Perceptron import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a proof to estimate the time of computation of the learning algorithm\n",
    "#the process will take = max seconds * 60. (6 grades and 10  epochs)\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = kernel_perceptron(x_train, x_dev, y_train, y_dev, 6, 10)\n",
    "print(\"--- %s max seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### HYPERPARAMETERS OPTIMIZATION ##########################################################\n",
    "#shuffle the training dataset\n",
    "union = np.concatenate((x_train, y_train), axis = 1)\n",
    "np.random.seed(223366)\n",
    "np.random.shuffle(union)\n",
    "result = []\n",
    "x_shuffled_train = union[:,:784]\n",
    "y_shuffled_train = union[:,784:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is used to compute test and training error, for each combination of epochs and grade of kernel, of both the predictors\n",
    "#computed using the two methods indicated (best binary predictor vs mean binary predictor)\n",
    "train_test_result_best = []\n",
    "train_test_result_mean = []\n",
    "for power in range(1,7):\n",
    "    power_result = []\n",
    "    for epoch in range(1,11):\n",
    "        train_test = kernel_perceptron(x_shuffled_train, x_dev, y_shuffled_train, y_dev, power, epoch)\n",
    "        power_result.append(train_test)\n",
    "    power_result = np.array(power_result)\n",
    "    power_result_best = power_result[:,0:2]\n",
    "    power_result_mean = power_result[:,2:4]\n",
    "    train_test_result_best.append(power_result_best)\n",
    "    train_test_result_mean.append(power_result_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of the test and training error of the predictor output by the learning algorithm, considering the best binary classifiers\n",
    "epoch = [1,2,3,4,5,6,7,8,9,10]\n",
    "#epoch 1, best\n",
    "plt.plot(epoch, train_test_result_best[0][:,0], color = \"g\", label = \"grade 1\")\n",
    "plt.plot(epoch, train_test_result_best[0][:,1], color = \"g\")\n",
    "\n",
    "#epoch 2, best\n",
    "plt.plot(epoch, train_test_result_best[1][:,0], color = \"b\", label = \"grade 2\")\n",
    "plt.plot(epoch, train_test_result_best[1][:,1], color = \"b\")\n",
    "\n",
    "#epoch 3, best\n",
    "plt.plot(epoch, train_test_result_best[2][:,0], color = \"k\", label = \"grade 3\")\n",
    "plt.plot(epoch, train_test_result_best[2][:,1], color = \"k\")\n",
    "\n",
    "#epoch 4, best\n",
    "plt.plot(epoch, train_test_result_best[3][:,0], color = \"r\", label = \"grade 4\")\n",
    "plt.plot(epoch, train_test_result_best[3][:,1], color = \"r\")\n",
    "\n",
    "#epoch 5, best\n",
    "plt.plot(epoch, train_test_result_best[4][:,0], color = \"y\", label = \"grade 5\")\n",
    "plt.plot(epoch, train_test_result_best[4][:,1], color = \"y\")\n",
    "\n",
    "#epoca 6, best\n",
    "plt.plot(epoch, train_test_result_best[5][:,0], color = \"c\", label = \"grade 6\")\n",
    "plt.plot(epoch, train_test_result_best[5][:,1], color = \"c\")\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Epochs')\n",
    "# naming the y axis\n",
    "plt.ylabel('Test and Training accuracy')\n",
    "# giving a title to my graph\n",
    "plt.title('Predictor_best_binary_classifiers')\n",
    " \n",
    "# show a legend on the plot\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "\n",
    "#save plot\n",
    "plt.savefig('best_bin_class.png', bbox_inches = 'tight')\n",
    "    \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of the test and training error of the predictor output by the learning algorithm, considering the mean binary classifiers\n",
    "epoch = [1,2,3,4,5,6,7,8,9,10]\n",
    "#epoca 1, best\n",
    "plt.plot(epoch, train_test_result_mean[0][:,0], color = \"g\", label = \"grade 1\")\n",
    "plt.plot(epoch, train_test_result_mean[0][:,1], color = \"g\")\n",
    "\n",
    "#epoca 2, best\n",
    "plt.plot(epoch, train_test_result_mean[1][:,0], color = \"b\", label = \"grade 2\")\n",
    "plt.plot(epoch, train_test_result_mean[1][:,1], color = \"b\")\n",
    "\n",
    "#epoca 2, best\n",
    "plt.plot(epoch, train_test_result_mean[2][:,0], color = \"k\", label = \"grade 3\")\n",
    "plt.plot(epoch, train_test_result_mean[2][:,1], color = \"k\")\n",
    "\n",
    "#epoca 2, best\n",
    "plt.plot(epoch, train_test_result_mean[3][:,0], color = \"r\", label = \"grade 4\")\n",
    "plt.plot(epoch, train_test_result_mean[3][:,1], color = \"r\")\n",
    "\n",
    "#epoca 2, best\n",
    "plt.plot(epoch, train_test_result_mean[4][:,0], color = \"y\", label = \"grade 5\")\n",
    "plt.plot(epoch, train_test_result_mean[4][:,1], color = \"y\")\n",
    "\n",
    "#epoca 2, best\n",
    "plt.plot(epoch, train_test_result_mean[5][:,0], color = \"c\", label = \"grade 6\")\n",
    "plt.plot(epoch, train_test_result_mean[5][:,1], color = \"c\")\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Epochs')\n",
    "# naming the y axis\n",
    "plt.ylabel('Test and Training  accuracy')\n",
    "# giving a title to my graph\n",
    "plt.title('Predictor_mean_binary_classifiers')\n",
    " \n",
    "# show a legend on the plot\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "\n",
    "#save_plot\n",
    "plt.savefig('Mean_bin_class.png', bbox_inches = 'tight')\n",
    " \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract from the two table containing the test and training error for both the predictor\n",
    "#the combination of epochs and grade of polynomial kernel the MAXIMIZE THE TEST ACCURACY\n",
    "search_best = np.array(train_test_result_best)\n",
    "search_mean = np.array(train_test_result_mean)\n",
    "ind_best = np.unravel_index(np.argmax(search_best[:,:,0], axis=None), search_best[:,:,0].shape)\n",
    "ind_mean = np.unravel_index(np.argmax(search_mean[:,:,0], axis=None), search_mean[:,:,0].shape)\n",
    "print(ind_best, ind_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################  TESTING PREDICTORS ##############################################################\n",
    "#########################################################################################################################################\n",
    "#get dummy for training label set\n",
    "yT = pd.get_dummies(yT)\n",
    "#change the type of the dataset\n",
    "yT = yT.astype('int8')\n",
    "#change from 0 to -1 the labels\n",
    "yT[yT != 1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dummy for test label set\n",
    "y_tst = pd.get_dummies(y_tst)\n",
    "#change the type of the dataset\n",
    "y_tst = y_tst.astype('int8')\n",
    "#change from 0 to -1 the labels\n",
    "y_tst[y_tst != 1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to np array, because of the function used to compute the predictor\n",
    "xT = xT.to_numpy()\n",
    "yT = yT.to_numpy()\n",
    "x_tst = x_tst.to_numpy()\n",
    "y_tst = y_tst.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the training set (excluding the labels)\n",
    "mean_px = xT.mean().astype(np.float32)\n",
    "std_px = xT.std().astype(np.float32)\n",
    "xT = (xT - mean_px)/(std_px)\n",
    "xT = np.float32(xT)\n",
    "\n",
    "#change no.type for test set\n",
    "x_tst = np.float32(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a smaller dataset to decrease the time of computation\n",
    "xTr = xT[:8000, :]\n",
    "x_tst = x_tst[:2500, :]\n",
    "yTr = yT[:8000, :]\n",
    "y_tst = y_tst[:2500, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain kernel percpetron and evaluate the performance of the ouptut predictor builted with the best binary classifiers\n",
    "#and considering the hyperparameters found in the first part\n",
    "accuracy1 = kernel_perceptron(xTr, x_tst, yTr, y_tst, (ind_best[0]+1), (ind_best[1]+1))\n",
    "print(accuracy1[0],accuracy1[1], accuracy1[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain kernel percpetron and evaluate the performance of the ouptut predictor builted with the mean binary classifiers\n",
    "#and considering the hyperparameters found in the first part\n",
    "accuracy2 = kernel_perceptron(xTr, x_tst, yTr, y_tst, (ind_mean[0]+1), (ind_mean[1]+1))\n",
    "print(accuracy2[2], accuracy2[3], accuracy2[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### METRICS ###################################################################\n",
    "#extract from the output of the kernel_percpetron the CONFUSION MATRIX\n",
    "confusion_matrix_best = accuracy1[4]\n",
    "confusion_matrix_mean = accuracy2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT the CONFUSION MATRIX computed from the prediction of the \"best_classifier\"\n",
    "cmap = sn.cm.rocket_r\n",
    "conf_1 = pd.DataFrame(confusion_matrix_best, range(10), range(10))\n",
    "sn.set(font_scale=1.3) # for label size\n",
    "sn.heatmap(conf_1, annot=True, annot_kws={\"size\": 13}, cmap = cmap, fmt='g' ) # font size\n",
    "\n",
    "#save plot\n",
    "plt.savefig('best_confusion_matrix.png', bbox_inches = 'tight')\n",
    "\n",
    "#show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT the CONFUSION MATRIX computed from the prediction of the \"mean_classifier\"\n",
    "cmap = sn.cm.rocket_r\n",
    "conf_2 = pd.DataFrame(confusion_matrix_mean, range(10), range(10))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(conf_2, annot=True, annot_kws={\"size\": 13}, cmap = cmap, fmt='g') # font size\n",
    "\n",
    "#save plot\n",
    "plt.savefig('mena_confusion_matrix.png', bbox_inches = 'tight')\n",
    "\n",
    "#plot show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute metrics \"RECALL\" and \"SPECIFICITY\" for best_classifier\n",
    "metric_best = metrics(confusion_matrix_best)\n",
    "metric_mean = metrics(confusion_matrix_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the four following cells are saved the barplot containing the information recall and precision for the two predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,1,2,3,4,5,6,7,8,9]\n",
    "y_precision_best = metric_best[0]\n",
    "clrs = clrs = ['green' if (precision > 0.95) else 'red'  if (precision < 0.85)  else 'grey' for precision in y_precision_best ]\n",
    "fig = sn.barplot(x, y_precision_best, palette=clrs) # color=clrs)\n",
    "fig.set(xlabel=\"Digit\", ylabel = \"Precision\") \n",
    "#save plot\n",
    "fig = fig.get_figure()\n",
    "fig.savefig('precision_best_classifier.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,1,2,3,4,5,6,7,8,9]\n",
    "y_recall_best = metric_best[1]\n",
    "clrs = clrs = ['green' if (precision > 0.95) else 'red'  if (precision < 0.825)  else 'grey' for precision in y_recall_best ]\n",
    "fig1 = sn.barplot(x, y_recall_best, palette=clrs) # color=clrs)\n",
    "\n",
    "fig1.set(xlabel=\"Digit\", ylabel = \"Recall\") \n",
    "#save plot\n",
    "fig1 = fig1.get_figure()\n",
    "fig1.savefig('recall_best_classifier.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,1,2,3,4,5,6,7,8,9]\n",
    "y_precision_mean = metric_mean[0]\n",
    "clrs = clrs = ['green' if (precision > 0.95) else 'red'  if (precision < 0.825)  else 'grey' for precision in y_precision_mean ]\n",
    "fig2 = sn.barplot(x, y_precision_mean, palette=clrs) # color=clrs)\n",
    "fig2.set(xlabel=\"Digit\", ylabel = \"Precision\") \n",
    "#save plot\n",
    "fig2 = fig2.get_figure()\n",
    "fig2.savefig('precision_mean_classifier.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,1,2,3,4,5,6,7,8,9]\n",
    "y_recall_mean = metric_mean[1]\n",
    "clrs = clrs = ['green' if (precision > 0.95) else 'red'  if (precision < 0.825)  else 'grey' for precision in y_recall_mean ]\n",
    "fig3 = sn.barplot(x, y_recall_mean, palette=clrs) # color=clrs)\n",
    "fig3.set(xlabel=\"Digit\", ylabel = \"Recall\") \n",
    "#save plot\n",
    "fig3 = fig3.get_figure()\n",
    "fig3.savefig('recall_mean_classifier.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING AND TEST ACCURACY VARYING THE NUMBER OF EXAMPLES IN THE TRAINING SET\n",
    "train_size = [2500, 5000, 7500, 10000, 15000]\n",
    "result = []\n",
    "for number in train_size:\n",
    "        train_tst = kernel_perceptron(xT[:number,:], x_tst, yT[:number,:], y_tst, ind_best[0]+1, ind_best[1]+1)\n",
    "        result.append(train_tst)\n",
    "result = np.array(result)\n",
    "result_best = result[:,0:2]\n",
    "result_mean = result[:,2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_size, result_best[:,0], color = \"g\", label = \"best\")\n",
    "plt.plot(train_size, result_best[:,1], color = \"g\")\n",
    "\n",
    "#mean\n",
    "plt.plot(train_size, result_mean[:,0], color = \"b\", label = \"mean\")\n",
    "plt.plot(train_size, result_mean[:,1], color = \"b\")\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Training_size')\n",
    "# naming the y axis\n",
    "plt.ylabel('Test and Training  accuracy')\n",
    " \n",
    "# show a legend on the plot\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "\n",
    "#save_plot\n",
    "plt.savefig('train_size.png', bbox_inches = 'tight')\n",
    " \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time for running the entire notebook is about 25 minute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
